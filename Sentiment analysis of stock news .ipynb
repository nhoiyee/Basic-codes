{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis of stock news "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing article data with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing is the process of analyzing a string of symbols, either in natural language, computer languages or data structures\n",
    "#goal: to get headlines of articles and run sentiment analysis on the text of those headlines to understand if everyday is positive or negative news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "tickers = ['AMZN','AMD','FB']\n",
    "for ticker in tickers:\n",
    "    url =finviz_url  +ticker\n",
    "\n",
    "    #request data from this url\n",
    "    req=Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response= urlopen(req)\n",
    "\n",
    "    html=BeautifulSoup(response,'html')\n",
    "    print(html) #pass html from finviz_url  \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see where the data lies, table holds the data and the table has id=\"news-table\"\n",
    "\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "tickers = ['AMZN','AMD','FB']\n",
    "news_tables={}\n",
    "for ticker in tickers:\n",
    "    url =finviz_url  +ticker\n",
    "\n",
    "    #request data from this url\n",
    "    req=Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response= urlopen(req)\n",
    "\n",
    "    html=BeautifulSoup(response,'html')\n",
    "    news_table=html.find(id='news_table') #get html objecet of the entire table\n",
    "    news_tables[ticker]= news_table #take table object and store it in dictionary\n",
    "\n",
    "    print(html) #pass html from finviz_url  \n",
    "    break\n",
    "    \n",
    "#the dictionary holds only the table of the results from the webpage: all table rows correspond to those in the webpage\n",
    "print(news_tables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulatie article data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see where the data lies, table holds the data and the table has id=\"news-table\"\n",
    "\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "tickers = ['AMZN','AMD','FB']\n",
    "news_tables={}\n",
    "for ticker in tickers:\n",
    "    url =finviz_url  +ticker\n",
    "\n",
    "    #request data from this url\n",
    "    req=Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response= urlopen(req)\n",
    "\n",
    "    html=BeautifulSoup(response,'html')\n",
    "    news_table=html.find(id='news_table') #get html objecet of the entire table\n",
    "    news_tables[ticker]= news_table #take table object and store it in dictionary\n",
    "\n",
    "    print(html) #pass html from finviz_url  \n",
    "    break\n",
    "    \n",
    "#the dictionary holds only the table of the results from the webpage: all table rows correspond to those in the webpage\n",
    "print(news_tables) \n",
    "\n",
    "#parse the data to understandable format to extract the titles and timestamps of the articles and apply on sentiment analysis \n",
    "#goal: to iterate all table rows in the dataset and get the values of timestamps and text of articles in the table\n",
    "#find all table rows that are relevant in the table html object\n",
    "#give a list of all the different tr elements inside the html object parsed in passed in the table of all the relevant news articles\n",
    "amzn_data =news_tables['AMZN']\n",
    "amzn_rows=amzn_data.findAll('tr')\n",
    "print(amzn_rows) \n",
    "\n",
    "#iterate over rows to get values\n",
    "for index, row in enumerate(amzn_rows):\n",
    "    title = row.a.text\n",
    "    print(title) \n",
    "\n",
    "#to get timestamps of data\n",
    "for index, row in enumerate(amzn_rows):\n",
    "    title = row.a.text\n",
    "    timestamp=row.td.text\n",
    "    print(timestamp + \" \" + title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "tickers = ['AMZN','AMD','FB']\n",
    "news_tables={}\n",
    "for ticker in tickers:\n",
    "    url =finviz_url  +ticker\n",
    "\n",
    "    #request data from this url\n",
    "    req=Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response= urlopen(req)\n",
    "\n",
    "    html=BeautifulSoup(response,'html')\n",
    "    news_table=html.find(id='news_table')\n",
    "    news_tables[ticker]= news_table \n",
    "\n",
    "    print(html)\n",
    "    break\n",
    "\n",
    "#list object, create list inside data set that correspond to ticker, date, time and title of article\n",
    "parsed_data=[]\n",
    "\n",
    "for ticker, news_table in news_tables.items():\n",
    "    for row in news_table.findAll('tr'):\n",
    "        title = row.a.get_text()\n",
    "        \n",
    "#split text in sections based on space: if length is only 1, it's just time, if more, there are multiple values- first is date then time\n",
    "        date_data = row.td.text.split(' ')\n",
    "        if len(date) == 1:\n",
    "            time=date_data [0]\n",
    "        else:\n",
    "            date = date_data [0]\n",
    "            time= date_data [1]\n",
    "        parsed_data.append([ticker,date,time,title])\n",
    "print(parsed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply sentiment analysis on headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply sentiment analysis on every title, use nltk vader sentiment\n",
    "import nltk \n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply sentiment analysis on any given text\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "tickers = ['AMZN','AMD','FB']\n",
    "news_tables={}\n",
    "for ticker in tickers:\n",
    "    url =finviz_url  +ticker\n",
    "\n",
    "    req=Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response= urlopen(req)\n",
    "\n",
    "    html=BeautifulSoup(response,'html')\n",
    "    news_table=html.find(id='news_table')\n",
    "    news_tables[ticker]= news_table \n",
    "\n",
    "    print(html)\n",
    "    break\n",
    "\n",
    "parsed_data=[]\n",
    "\n",
    "for ticker, news_table in news_tables.items():\n",
    "    for row in news_table.findAll('tr'):\n",
    "        title = row.a.get_text()\n",
    "        \n",
    "        date_data = row.td.text.split(' ')\n",
    "        if len(date) == 1:\n",
    "            time=date_data [0]\n",
    "        else:\n",
    "            date = date_data [0]\n",
    "            time= date_data [1]\n",
    "parsed_data.append([ticker,date,time,title])\n",
    "\n",
    "#create dataframe to host our data in\n",
    "df = pd.DataFrame(parsed_data, columns=['ticker', 'date','time','title'])\n",
    "\n",
    "print(df.head())\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "print(vader.polarity_scores(\"I don't think the weather is going to be good for hiking today.\"))\n",
    "\n",
    "print(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vader.polarity_scores(\"I think the weather is going to be good for skiing today.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(parsed_data, columns=['ticker', 'date','time','title'])\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compiund']\n",
    "#create column 'compound' \n",
    "df['compound']=df['title'].apply(f)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compiund']\n",
    "df['compound']=df['title'].apply(f)\n",
    "\n",
    "#to convert date from normal string to date time format\n",
    "df['date'] = pd.to_datetime(df.date).dt.date\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "mean_df=df.groupby(['ticker'],['date']).mean()\n",
    "print(mean_df) #mean to get the average sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "mean_df=df.groupby(['ticker'],['date']).mean()\n",
    "\n",
    "#unstack to get date at x axis\n",
    "mean_df=mean_df.unstack()\n",
    "mean_df=mean_df.xs('compound', axis=\"columns\").transpose()\n",
    "mean_df.plot(kind='bar')\n",
    "print(mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c5f18f9a703a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnews_table\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews_tables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "tickers = ['AMZN','GOOG','FB']\n",
    "news_tables={}\n",
    "for ticker in tickers:\n",
    "    url =finviz_url+ticker\n",
    "\n",
    "    req=Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response= urlopen(req)\n",
    "\n",
    "    html=BeautifulSoup(response,'html')\n",
    "    news_table=html.find(id='news_table')\n",
    "    news_tables[ticker]= news_table \n",
    "\n",
    "parsed_data =[]\n",
    "\n",
    "for ticker, news_table in news_tables.items():\n",
    "    \n",
    "    for row in news_table.findAll('tr'):\n",
    "        \n",
    "        title = row.a.text\n",
    "        date_data = row.td.text.split(' ')\n",
    "        \n",
    "        if len(date_data) == 1:\n",
    "            time=date_data[0]\n",
    "        else:\n",
    "            date = date_data[0]\n",
    "            time= date_data[1]\n",
    "        parsed_data.append([ticker,date,time,title])\n",
    "\n",
    "df = pd.DataFrame(parsed_data, columns=['ticker', 'date','time','title'])\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compiund']\n",
    "df['compound']=df['title'].apply(f)\n",
    "df['date'] = pd.to_datetime(df.date).dt.date\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "mean_df=df.groupby(['ticker'],['date']).mean()\n",
    "\n",
    "mean_df=mean_df.unstack()\n",
    "mean_df=mean_df.xs('compound', axis=\"columns\").transpose()\n",
    "mean_df.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
